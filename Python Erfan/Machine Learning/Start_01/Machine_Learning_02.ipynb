{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzBdUUbhCzzS"
   },
   "source": [
    "# Machine Learning: Supervised Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aghI3aKnKroh"
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkaiWA7GC3ng"
   },
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5FCSMiBC-n1"
   },
   "source": [
    "> Classification: where you output variable y can take on only one of a small handful of possible values instead of any number in an infinite range of numbers\n",
    "\n",
    "Some classification example:\n",
    "* Is this email spam? = $y$ > No/Yes\n",
    "* Is the transaction fraudulent? = $y$ > No/Yes\n",
    "* Is the tumor maligant? = $y$ > No/Yes\n",
    "\n",
    "> Note: $y$ can only be one of two values (it's called binary classification)\n",
    "\n",
    "> NOTE: Class = Category\n",
    "\n",
    "we often designate clauses as **NO** or **Yes** or sometimes equivalently **False** or **True** or very commonly using the numbers **0** or **1**\n",
    "\n",
    ">NOTE:\\\n",
    "0 =>**Negative class**\\\n",
    "1 => **Positive Class**\n",
    "\n",
    "We can't use linear regression, it's predicts not just values 0 and 1 but all number between 0 and 1 or even less than 0 or greater than 1,\n",
    "but here we want to predict categories,\n",
    "But HOW?\n",
    "1. Pick a threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCQ_qEgrw8un"
   },
   "source": [
    "### Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bs86LoRaxFqh"
   },
   "source": [
    "> NOTE: It's the single most widely used classification algorithm in the world.\n",
    "\n",
    "Let's work with a example we whant to classifying whether a tumor is malignant at this example we have:\n",
    "* two lables (Yes/No)(1/0)\n",
    "* some result on 0 and some are on 1\n",
    "\n",
    "Horizontal axis is the tumor size and vertical axis takes on only values of 0 and 1 because it's a classification problem\\\n",
    "We know linear regression is not a good algorithm for this problem\n",
    "> What logistic regression doing?\\\n",
    "fit a curve that look like across all the points (it's like a S shape)\n",
    "\n",
    "! We want output between 0 and 1\n",
    "\n",
    "to build up to the logistic regression algorithm there's an important mathematical function I'd like to describe which is called the **sigmoid function** also call it **logistic function**\n",
    "> NOTE: the sigmoid function outputs values between 0 and 1\n",
    "\n",
    "If we use G and Z to denote this function:\n",
    "> Formula: $g(z) = \\frac{1}{1 + e^{-z}}$   0 < $g(z)$ < 1\n",
    "\n",
    "$e$ is a mathematical constant that takes on a value of about 2.7 and so $e^{-z}$ is that mathematical constant to the power of negative $Z$\n",
    "> NOTE: if $z$ were really big $e^{-z}$ is go to be a tiny tiny number\n",
    "\n",
    "> NOTE: if $z$ = 0 the $e^{-z}$ = $e^{0}$ which is equal to 1 so $g(z)$ = 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdaiCtXWmLCv"
   },
   "source": [
    "Now let's use this to build up to the logistic regression algorithm, we're going to do this in two steps:\n",
    "1. first you need to remember linear regression function :\\\n",
    "$f_{W,b}(X) = W.X + b$\\\n",
    "we can store this value in a variable which I'm going to call $z$ ans this will turn out to be tha same $z$ as the one you saw on on previous we taked.\n",
    "2. Next step is to take this value of $z$ and pass it to the sigmoid function (logistic function)\n",
    "\n",
    "Now if we put eveything next to other:\n",
    "> Logistic regression model:\\\n",
    "$f_{W,b}(X) = g(W.X + b) = \\frac{1}{1 + e^{-(W.X+b)}}$\\\n",
    "!! $(W.X+b) = z$\n",
    "\n",
    "What it does is it inputs a feature or set a feature x and it outputs a number between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ny_Q-KMrVuh"
   },
   "source": [
    "Let's take look at how to interpret the output of logistic regression:\n",
    "\n",
    "> MEMORY: $f_{W,b}(X) = \\frac{1}{1 + e^{-(W.X+b)}}$\n",
    "\n",
    "> NOTE: the way you to think of logistice regressions output is thick of it as outputting the probability that the cost or label y will be equal to 1 given a certain input X\n",
    "\n",
    "Let's continue with last example about tumor size:\\\n",
    "Example:\\\n",
    "$x$ is \"tumor size\"\\\n",
    "$y$ is 0 (not malignant) or 1 (malignant)\\\n",
    "\n",
    "if a patient come in and she\\he has a tumor of a certain size x:\\\n",
    "$f_{W,b}(X) = 0.7$\\\n",
    "that means is that the model is preficting or the model thinks there is a 70% chance that the true lable y will be equal to 1 for this patient\n",
    "\n",
    "so if y has a 70% chance of being one what is the chance that it is 0?\\\n",
    "> EXPLAIN: $P(y = 0) + P(y=1) = 1$\n",
    "\n",
    "so for our example the chance of it being zero has got to be 0.3 or 30% chance\n",
    "> $f_{W,b}(X) = P(y = 1|X;W,b)$\\\n",
    "; in here means W and b are parameters that affect this computation of what is the probability of Y being equal to 1 given the input feature X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uJNR6Bt6y80"
   },
   "source": [
    "### Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1XDzSNLSA0O"
   },
   "source": [
    "#### Linear decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E11Kw23T66LZ"
   },
   "source": [
    "> EXPLAIN: how logistic regression is computing is predictions\n",
    "\n",
    "for recap:\\\n",
    "$f_{W,b}(X)$\\\n",
    "$z = W . X + b$\\ -> Z -> $g(z) = \\frac{1}{1 + e^{-z}}$\\\n",
    "Another way to write this is:\\\n",
    "$f_{X,b}(X) = g(W.X + b)$\n",
    "\n",
    "> Formula:\\\n",
    "$f_{W,b}(X) = g(W.X + b) = \\frac{1}{1 + e^{-(W.X+b)}} = P(y = 1|x;W,b)$\\\n",
    "!! $(W.X+b) = z$\n",
    "\n",
    "What if you want the learning algorithm to predict is the value of y going to be 0 or 1?\\\n",
    "One way we can us threshold:\\\n",
    "Is $f_{W,b}(X) >= 0.5$ Yes:y-hat = 1/No:y-hat = 0\n",
    "\n",
    "When is $f_{W,b}(X) >= 0.5?$ it's happend when $g(z) >= 0.5$\\\n",
    "and it's happend when $z >= 0$ (when $z$ is on the right half of this axis)\\\n",
    "$z$ is greater than equal to zero whenever $(W.X + b) >= 0$\\\n",
    "what we told is the model predicts one\n",
    "\n",
    "If $W.X + b < 0$ the y-hat = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mzGqc4U-xPZ"
   },
   "source": [
    "Let's work with a example:\\\n",
    "Think we have a classification problem where you have two features X1 and X2\\\n",
    "We have a chart and both way until 3, we have training set with the little red crosses denote the positive examples and little blue circle denote negative examples\\\n",
    "so the red crosses corresponds to y = 1 and blue circles correspond to y = 0\\\n",
    "so the logistic regression model will make predictions using this function:\\\n",
    "$f_{W,b}(X) = g(z)$\\\n",
    "and now because of z:\\\n",
    "$f_{W,b}(X) = g(w_{1}x_{1} + w_{2}x_{2} + b)$\\\n",
    "Let's just say for this example that the value of the parameters are $w_{1}$ = 1, $w_{2} = 1$ and $b$ = -3\\\n",
    "Let's figure out when $(W.X + b)$ is greater than or equal to 0 and when $(W.X + b)$ is less than zero\n",
    "\n",
    "> NOTE: when $W.X + b$ is exactly equal to 0 it turns out that this line is also called the decision boundary because that's the line where you're just almost neutral about whether y is 0 or y is 1\n",
    "\n",
    "for the values of the parameters $w_{1}$ and $w_{2}$ and $b$ that we had written down above this decision boundary is just:\\\n",
    "$z = x_{1} + x_{2} - 3 = 0$\\\n",
    "that will correspond to the line:\\\n",
    "$x_{1} + x_{2} = 3$\\\n",
    "and that show the spefic line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZANmMV9SL9j"
   },
   "source": [
    "#### Non-linear decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpO7pyB6SQeM"
   },
   "source": [
    "Let's look at another example for Non-linear\\\n",
    "as befor crosses denote the class y = 1 and the little circles denote the clause y = 0\\\n",
    "we set z like this:\\\n",
    "$z = w_{1}x_{1}^{2} + w_{2}x_{2}^{2} + b$\n",
    "with this choice of features polynomial features into a logistic regression so:\\\n",
    "$f_{W,b}(X) = g(z) = g(w_{1}x_{1}^{2} + w_{2}x_{2}^{2} + b)$\\\n",
    "and let's say that we end up choosing $w_{1}$ and $w_{2}$ to be 1 and $b$ to be -1 so:\\\n",
    "$z = x_{1}^{2} + x_{2}^{2} - 1$\\\n",
    "and decision boundary as befor will correspond to when $z$ is equal to 0 and so this expression will be equal to 0 when $x_{1}^{2} + x_{2}^{2} = 1$\\\n",
    "When $x_{1}^{2} + x_{2}^{2} >= 1$ y-hat = 1\\\n",
    "When $x_{1}^{2} + x_{2}^{2} < 1$ y-hat = 0\n",
    "\n",
    "Also we can have more complex examples like:\\\n",
    "$f_{W,b}(X) = g(z) = g(w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{1}^{2} + w_{4}x_{1}x_{2} + w_{5}x_{2}^{2} + w_{6}x_{1}^{3} + ... + b)$\\\n",
    "when we create shape with that:\\\n",
    "y=hat = 1: means inside shape\\\n",
    "y-hat = 0: means outside shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oh96rpEzmcIC"
   },
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bG3DSt4mfYQ"
   },
   "source": [
    "#### Cost function for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvqO9zljmqK1"
   },
   "source": [
    ">REMEMBER: cost function gives you a way to measure how well a speific set of parameters fits the training data and it thereby gives you a way to try to choose better parameters\n",
    "\n",
    "! squared error cost function is not an Id cost function for religious regression\n",
    "\n",
    "Training set like that:\\\n",
    "Input's = tumor size, ..., patient's age\\\n",
    "Output's = malignant? (1:Yes/2:No)\n",
    "> NOTE:\\\n",
    "m = training examples (i = 1, ...,)\\\n",
    "n = features (j = 1, ...,)\\\n",
    "y = target, is 0 or 1\n",
    ">> Logistic regression model:\\\n",
    "$f_{w^{â†’},b}(x^{â†’}) = \\frac{1}{1 + e^{-(w^{â†’}.x^{â†’} + b)}}$\n",
    "\n",
    "How to choose $w^{â†’} = [w_{1}, w_{2}, w_{3}, ..., w_{n}]$ ?\n",
    "\n",
    "> MEMORY:\\\n",
    "Squared error cost:\\\n",
    "$J(w^{â†’},b) = \\frac{1}{m}Î£_{i = 1}^{m}\\frac{1}{2}(f_{w^{â†’},b}(x^{â†’(i)})-y^{i})^{2}$\\\n",
    "and we have linear regression:\\\n",
    "$f_{w^{â†’},b}(x^{â†’}) = (w^{â†’}.x^{â†’} + b)$\\\n",
    "and we take a one step one step and so on to converge at the global minimum\n",
    "\n",
    "but if you try **Squared error cost** you can see it dosen't works for **Logistic regression** and you recive to a non-convex cost function it's not context and it's means is that if you were to try to use gradient descent there are lot's of local minimum that you can get stuck in.\\\n",
    "We need to use different cost function that can make cost function convex again so the gradient descent can be guaranteed to converge to the global minimum.\\\n",
    "What we do:\\\n",
    "We're going to change a bit the definition of the cost function J of w and b\\\n",
    "We take inside of it and call it **loss**:\n",
    "loss[$L(\\frac{1}{2}(f_{w^{â†’},b}(x^{â†’(i)})-y^{(i)}$]\\\n",
    "The loss on a single trainig example and I'm going to denote the loss via this L and is a function of the prediction of the learning algorithm f of x as well as of the true label y.\n",
    "> NOTE: Chose different form for this loss function we'll be able to keep the overall cost function which is 1 over m times the sum of these loss functions to be a convex function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCdiDggzFqsT"
   },
   "source": [
    "Definition of the loss function we'll use for logistic regression:\n",
    "> Formula:\\\n",
    "$L(f_{w^{â†’},b}(x^{â†’(i)}), y^{(i)} = $\\\n",
    "1: $-log(f_{w^{â†’},b}(x^{â†’(i)})$  if $y^{(i)} = 1$\\\n",
    "2: $-log(1 - f_{w^{â†’},b}(x^{â†’(i)})$  if $y^{(i)} = 0$\n",
    "\n",
    "Let's see what happend at case 1:\\\n",
    "$L(f_{w^{â†’},b}(x^{â†’(i)}), y^{(i)}$\\\n",
    "if $y^{i} = 1$\\\n",
    "As $f_{w^{â†’},b}(x^{â†’(i)})$ -> 1 then **loss** -> 0 ðŸ™‚\\\n",
    "As $f_{w^{â†’},b}(x^{â†’(i)})$ -> 0 then **loss** -> âˆž ðŸ˜ž\n",
    "> NOTE: Loss is lowest when $f_{w^{â†’},b}(x^{â†’(i)})$ predict close to true label $y^{(i)}$\\\n",
    "Let's see what happend at case 2:\\\n",
    "$L(f_{w^{â†’},b}(x^{â†’(i)}), y^{(i)}$\\\n",
    "if $y^{i} = 0$\\\n",
    "As $f_{w^{â†’},b}(x^{â†’(i)})$ -> 0 then **loss** -> 0 ðŸ™‚\\\n",
    "As $f_{w^{â†’},b}(x^{â†’(i)})$ -> 1 then **loss** -> âˆž ðŸ˜ž\\\n",
    "> NOTE: The further prediction $f_{w^{â†’},b}(x^{â†’(i)})$ is from target $y^{(i)}$, the higher the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2S3LYq8WJ_ZK"
   },
   "source": [
    "Cost:\\\n",
    "$J(w^{â†’},b) = \\frac{1}{m}Î£_{i = 1}^{m}L(f_{w^{â†’},b}(x^{â†’(i)}),y^{i})$\\\n",
    "**Loss** = $f_{w^{â†’},b}(x^{â†’(i)}),y^{i})$\\\n",
    "and it's equal with 2 cases:\\\n",
    "1: $-log(f_{w^{â†’},b}(x^{â†’(i)})$  if $y^{(i)} = 1$\\\n",
    "2: $-log(1 - f_{w^{â†’},b}(x^{â†’(i)})$  if $y^{(i)} = 0$\\\n",
    "and if you find the value of parameters w and b that minimize this then you'd have a pretty good set of values for the parameters w and b for logistic regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdeNSCWB9xBk"
   },
   "source": [
    "#### Simplified Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3xTVVXb975c"
   },
   "source": [
    "We expline a slightly simpler way to write out the loss and cost function\n",
    "> Lost function for logistic regression\\\n",
    "$L(f_{w^{â†’},b}(x^{â†’(i)}), y^{(i)} = $\\\n",
    "1: $-log(f_{w^{â†’},b}(x^{â†’(i)})$  if $y^{(i)} = 1$\\\n",
    "2: $-log(1 - f_{w^{â†’},b}(x^{â†’(i)})$  if $y^{(i)} = 0$\n",
    "\n",
    "because y is either 0 or 1 and cannot take on any value other than zero or one, we'll be able to come up with a simpler way to write this loss function:\n",
    "> Simpler loss function:\\\n",
    "$L(f_{w^{â†’},b}(x^{â†’(i)}), y^{(i)} = -y^{(i)}log(f_{w^{â†’},b}(x^{â†’(i)})) - (1 - y^{(i)}log(1 - f_{w^{â†’},b}(x^{â†’(i)}))$\n",
    "\n",
    "Let's see what happend for $y^{(i)} = 1$ and $y^{(i)} = 0$\\\n",
    "if $y^{(i)} = 1$:\\\n",
    "$L(f_{w^{â†’},b}(x^{â†’(i)}), y^{(i)}) = -1log(g(x^{â†’}))$\\\n",
    "if $y^{(i)} = 0$:\\\n",
    "$L(f_{w^{â†’},b}(x^{â†’(i)}), y^{(i)}) = -log(1 - f_{w^{â†’},b}(x^{â†’(i)})$  \n",
    "\n",
    "> Cost function for logistic regression:\\\n",
    "First we have simplified cost function:\\\n",
    "$L(f_{w^{â†’},b}(x^{â†’(i)}), y^{(i)} = -y^{(i)}log(f_{w^{â†’},b}(x^{â†’(i)})) - (1 - y^{(i)}log(1 - f_{w^{â†’},b}(x^{â†’(i)}))$\\\n",
    "Then we write cost function like below:\\\n",
    "$J(w^{â†’},b) = \\frac{1}{m}Î£_{i = 1}^{m}[L(f_{w^{â†’},b}(x^{â†’(i)})-y^{i})]$\\\n",
    "And finally **Cost function for logistic** is:\\\n",
    "$-\\frac{1}{m}Î£_{i = 1}^{m}[y^{(i)}log(f_{w^{â†’},b}(x^{â†’(i)})) + (1 - y^{(i)}log(1 - f_{w^{â†’},b}(x^{â†’(i)}))]$\n",
    "\n",
    "Why do we choose this particular function, when there could be tons of other cost functions we could have chosen?\\\n",
    "This particular cost function is derived from statistics using a statistical principle called maximum likelihood estimation, which is idea from statistic on how to efficiently find parameters for different models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hqg_Vpot_7G"
   },
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-ONpU0guHJw"
   },
   "source": [
    "#### Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YJ9rwxvuVdJ"
   },
   "source": [
    "In here we focus on how to find a good choice of the parameters $w^{â†’}$ and  $b$\\\n",
    "Given new $x^{â†’}$, output $f_{w^{â†’},b}(x^{â†’}) = \\frac{1}{1 + e^{-(w^{â†’}.x^{â†’}+ b)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSd7MTWSvWtE"
   },
   "source": [
    "Diagram you can use to minimize the cost function is **gradient descent**\\\n",
    "here again is the **cost function**:\n",
    "\n",
    "$J(w^{â†’},b) = -\\frac{1}{m}Î£_{i = 1}^{m}[y^{(i)}log(f_{w^{â†’},b}(x^{â†’(i)})) + (1 - y^{(i)}log(1 - f_{w^{â†’},b}(x^{â†’(i)}))]$\n",
    "\n",
    "if you want to minimize the cost J as a function of w and b, the ususal gradient descent algorithm is:\n",
    "\n",
    "repeat {\\\n",
    "$w_{j} = w_{j} - Î±\\frac{âˆ‚}{âˆ‚w_{j}}J(w^{â†’},b)$\\\n",
    "$b = b - Î±\\frac{âˆ‚}{âˆ‚b}J(w^{â†’},b)$\\\n",
    "}\n",
    "\n",
    "learning rate times this dericative term let's take a look at the dericative of J with respect to w,J\\\n",
    "as usual J goes from 1 throught n where n is the number of features\n",
    "\n",
    "we know:\\\n",
    "$\\frac{âˆ‚}{âˆ‚w_{j}}J(w^{â†’},b) = \\frac{1}{m}Î£_{i = 1}^{m}(f_{w^{â†’},b}(x^{â†’(i)})-y^{i})x_{j}^{(i)}$\\\n",
    "and\\\n",
    "$\\frac{âˆ‚}{âˆ‚b}J(w^{â†’},b) = \\frac{1}{m}Î£_{i = 1}^{m}(f_{w^{â†’},b}(x^{â†’(i)})-y^{i})$\n",
    "> REMINDER: similar to what you saw for linear regression the way to carry out these updates is to use simultaneous updates meaning that you would first compute the right hand sign for all of these updates and then simultaneously overwrite all the values on the left at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JA8v3Zx_9DRx"
   },
   "source": [
    "#### Gradient descent for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2cnnfT-9RDL"
   },
   "source": [
    "> Formula:\\\n",
    "repeat {\\\n",
    "$w_{j} = w_{j} - Î±[\\frac{1}{m}Î£_{i = 1}^{m}(f_{w^{â†’},b}(x^{â†’(i)})-y^{(i)})x_{j}^{(i)}]$\\\n",
    "$b = b - Î±[\\frac{1}{m}Î£_{i = 1}^{m}(f_{w^{â†’},b}(x^{â†’(i)})-y^{(i)})]$\\\n",
    "} simultaneous update\n",
    "\n",
    "Same concepts with linear regression:\\\n",
    "* Monitor gradient descent\n",
    "* Vecrotized implementation\n",
    "* Feature scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWMCVQKlQmTp"
   },
   "source": [
    "### Regularization to reduce overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fV_U9CD4Qse0"
   },
   "source": [
    "#### The Problem of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5ueH4yuRZgB"
   },
   "source": [
    "Sometimes in an application the album could run into a problem called overfitting which can cause it to perform poorly\n",
    "\n",
    "> EXPLAIN: when our datas does not fit the training set well, we call it underfit(high bias)\\\n",
    "$w_{1}x + b$\n",
    "\n",
    "if you're a real estate agent the idea that you want your learning algorithm to do well even on examples that are not on the training set that's called generalization\\\n",
    "technically we say that you want your learning algorithm to generalize well witch means to make good predictions even on brand new example that has never seen befor\n",
    "\n",
    "> EXPLAIN: when training set pretty well we say generalization\\\n",
    "$w_{1}x + w_{2}x^{2} + b$\n",
    "\n",
    "> EXPLAIN: when fits the training set extremely well we say overfit or high variance(bad at predictions)\\\n",
    "$w_{1}x + w_{2}x^{2} + w_{3}x^{3} + w_{4}x^{4} +b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjC_79BbY9Wx"
   },
   "source": [
    "#### Addressing Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxcsrXhvZGmR"
   },
   "source": [
    "Let's say you fit a model and it has high variance is overfit\\\n",
    "one way to address this problem is to collect more training data so that's one option, if you able to get more data, that is more training examples, then with the larger traning set the learning algorithm will learn fit a function that is less wigly so you can continue to fit a high order polinomial or some of the function with a lot of feature and if you have enouth training examples it will still do okay.\n",
    "\n",
    "The number one tool you can use against overfitting is to get more training data, but it's not always a good option.\\\n",
    "Second option for adressing overfitting is to see if you can use fewer features\n",
    "\n",
    "> NOTE: all features + insufficient data -> overfit\n",
    "\n",
    "Pick just a subset of the most useful ones:\\\n",
    "for example for pric of house just pick: size, bedrooms and age of the house\\\n",
    "if you think those are the most relevant features then using just that smaller subset of features you may find that your model no longer overfits as badly\n",
    "\n",
    "choosing the most appropriate set of features to use is sometimes also called feature selection, one way you could do so is to use your intuition to choose what you think is the best set of features, what's most relevant for predicting the price.\\\n",
    "But what is the disadvantage of choosing features?\\\n",
    "by using only a subset of the features the algorithm is throwing away some of the information that you have about your data and they can be actually useful for predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paeidMobqMeK"
   },
   "source": [
    "#### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbnUHmxaqjOP"
   },
   "source": [
    "This is a third option for reducing overfitting\\\n",
    "regularization is a way to more gently reduce the impacts of some of the features without doing something as harsh as eliminating it outright\n",
    "\n",
    "> Regularization:\\\n",
    "is encourage the learning algorithm to shrink the values of the parameters without necessarily demanding that the parameters is set to exactly zero and it turns out that even if you fit a higher order polinomial like this so long as you can get the algorith to use smaller parameter values $w_{1}$, $w_{2}$, $w_{3}$,$w_{4}$ (for when you have 4 x), you end up with a curve that ends up fitting the training data much better\n",
    "\n",
    "> What regularization dose?\\\n",
    "it let's keep all of your features but it just prevents the features from having a overly large effect, witch is what sometimes can cause overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FI-ft8bLFGO-"
   },
   "source": [
    "#### Cost Function With Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlujCI0JG0yo"
   },
   "source": [
    "Let's say instead of minimizing this objective function\\\n",
    "$min_{w^{â†’},b}\\frac{1}{2m}Î£_{i=1}^{m}(f_{w^{â†’},b}(x^{â†’(i)} - y^{(i)})^{2})$\\\n",
    "above, this is the cost fuction for linear regression\\\n",
    "let's say you were to modify the cost function and add to it like this:\\\n",
    "$min_{w^{â†’},b}\\frac{1}{2m}Î£_{i=1}^{m}(f_{w^{â†’},b}(x^{â†’(i)} - y^{(i)})^{2}) +1000w_{3}^{2} + 1000w_{4}^{2}$\\\n",
    "with this modified cost function you in effect be penalizing the model if w3 and w4 are large because if you want minimize this function the only way to make this new cost function small is if w3 and w4 are both samll\\\n",
    "So when you minimize this function you're going to end up with w3 close to 0 and w4 close to 0 so we're effectively nearly canceling out the effect of the features execute and x to the power of 4 and getting rid of these two terms over here and if we do that then we end up with a fit to the data that's much closer to the quadratic function including meybe just tiny contributions from the feature x cubed and x to the 4.\\\n",
    "and this is good because it's a mush better fit to the data compared to if all the parameters could be large and you end up with this weekly quadratic function more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjTTceJcKHtU"
   },
   "source": [
    "Generally here's the idea behind regularization:\\\n",
    "if there are smaller values for the parameters then that's a bit like having a simpler model maybe one with fewer features which is therefore less prone to overfitting\\\n",
    "The way that regularization tends to be implemented is, if you have a lot of features say 100 features you may not know which are the most important features and which ones to penalize so the way regularization is typically implemented is to penalize all of the features or more precisely you penalize all the wj parameters and it's possible to show that this will usually result in fitting a smoother simpler less weekly function that's less prone to overfitting.\n",
    "> $J(w^{â†’},b) = min_{w^{â†’},b}\\frac{1}{2m}Î£_{i=1}^{m}(f_{w^{â†’},b}(x^{â†’(i)} - y^{(i)})^{2}) + \\frac{Î»}{2m}Î£_{j = 1}^{n}w_{j}^{2}$\n",
    "\n",
    "and sometimes:\n",
    "\n",
    "> $J(w^{â†’},b) = min_{w^{â†’},b}\\frac{1}{2m}[Î£_{i=1}^{m}(f_{w^{â†’},b}(x^{â†’(i)} - y^{(i)})^{2}) + \\frac{Î»}{2m}Î£_{j = 1}^{n}w_{j}^{2} + \\frac{Î»}{2m}b^{2}]$\\\n",
    "this makes very little difference in practice and the more common convention\n",
    "\n",
    "What we want:\\\n",
    "> $min_{w^{â†’,b}} J(w_{â†’},b) = min_{w^{â†’},b}[\\frac{1}{2m}Î£_{i=1}^{m}(f_{w^{â†’},b}(x^{â†’(i)} - y^{(i)})^{2}) + \\frac{Î»}{2m}Î£_{j = 1}^{n}w_{j}^{2}]$\\\n",
    "\n",
    "Two goals that you might have:\\\n",
    "1. trying to minimize this first term encourages the algorithm to fit the training data well by minimizing the square differences of the predictions and the actual values\n",
    "2. try to minimize the second term ($\\frac{Î»}{2m}Î£_{j = 1}^{n}w_{j}^{2}$) the algorithm also tries to keep the parameters wj small which will tend to reduce overfitting\n",
    "\n",
    "the value of $Î»$ that you choose specifies the relative importance or the relative trade-off or how you balance between these two goals.\n",
    "\n",
    "> if $Î»$ was set to be zero then you're not using the regularization term at all because the reqularization term is multiplied by zero and so if $Î»$ was zero you end up fitting this overly weekly overly complex curve and it overfits\n",
    "\n",
    "> if $Î»$ be a very large number, then you're placing a very heavy weight on this regularization term on the right and the only way to minimize this is to be sure that all the values of w are pretty much very close to zero so if $Î»$ is very very large the learning algorithm will xhoose w1, w2, w3 and w4 to be extremely close to zero and thus $f(x) = b$ and so the learning algorithm fits a horizontal straight line and underfits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2y5ALD5PQsAe"
   },
   "source": [
    "#### regularized Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how gradient descent to work with regularized linear regression\n",
    "> Cost Function for regularized linear regression\\\n",
    "> $J(w^{â†’},b) = \\frac{1}{2m}Î£_{i=1}^{m}(f_{w^{â†’},b}(x^{â†’(i)}) - y^{(i)})^{2} + \\frac{Î»}{2m}Î£_{j=1}^{n}w_{j}^{2}$\n",
    "\n",
    "the first part is the usual squared error cost function\\\n",
    "then we have lambda is the reqularization parameter and you'd like to find parameters w and b and what we want is:\n",
    "> $min_{w^{â†’,b}} J(w_{â†’},b) = min_{w^{â†’},b}[\\frac{1}{2m}Î£_{i=1}^{m}(f_{w^{â†’},b}(x^{â†’(i)} - y^{(i)})^{2}) + \\frac{Î»}{2m}Î£_{j = 1}^{n}w_{j}^{2}]$\n",
    "\n",
    "and we have the following gradient algorithm\n",
    "> repeat {\n",
    ">\n",
    ">}\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOlRkH4ApgUhUgFTRy7zhRu",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Erfan_MyPython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
