{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXoG6Bh3zpvP1Vg3SOhKix",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ErfanEbFiN79/Erfan-Python/blob/main/Python%20Erfan/Machine%20Learning/Start_01/Machine_Learning_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning: Supervised Learning\n"
      ],
      "metadata": {
        "id": "OzBdUUbhCzzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification"
      ],
      "metadata": {
        "id": "aghI3aKnKroh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Motivation"
      ],
      "metadata": {
        "id": "zkaiWA7GC3ng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Classification: where you output variable y can take on only one of a small handful of possible values instead of any number in an infinite range of numbers\n",
        "\n",
        "Some classification example:\n",
        "* Is this email spam? = $y$ > No/Yes\n",
        "* Is the transaction fraudulent? = $y$ > No/Yes\n",
        "* Is the tumor maligant? = $y$ > No/Yes\n",
        "\n",
        "> Note: $y$ can only be one of two values (it's called binary classification)\n",
        "\n",
        "> NOTE: Class = Category\n",
        "\n",
        "we often designate clauses as **NO** or **Yes** or sometimes equivalently **False** or **True** or very commonly using the numbers **0** or **1**\n",
        "\n",
        ">NOTE:\\\n",
        "0 =>**Negative class**\\\n",
        "1 => **Positive Class**\n",
        "\n",
        "We can't use linear regression, it's predicts not just values 0 and 1 but all number between 0 and 1 or even less than 0 or greater than 1,\n",
        "but here we want to predict categories,\n",
        "But HOW?\n",
        "1. Pick a threshold\n"
      ],
      "metadata": {
        "id": "s5FCSMiBC-n1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "UCQ_qEgrw8un"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> NOTE: It's the single most widely used classification algorithm in the world.\n",
        "\n",
        "Let's work with a example we whant to classifying whether a tumor is malignant at this example we have:\n",
        "* two lables (Yes/No)(1/0)\n",
        "* some result on 0 and some are on 1\n",
        "\n",
        "Horizontal axis is the tumor size and vertical axis takes on only values of 0 and 1 because it's a classification problem\\\n",
        "We know linear regression is not a good algorithm for this problem\n",
        "> What logistic regression doing?\\\n",
        "fit a curve that look like across all the points (it's like a S shape)\n",
        "\n",
        "! We want output between 0 and 1\n",
        "\n",
        "to build up to the logistic regression algorithm there's an important mathematical function I'd like to describe which is called the **sigmoid function** also call it **logistic function**\n",
        "> NOTE: the sigmoid function outputs values between 0 and 1\n",
        "\n",
        "If we use G and Z to denote this function:\n",
        "> Formula: $g(z) = \\frac{1}{1 + e^{-z}}$   0 < $g(z)$ < 1\n",
        "\n",
        "$e$ is a mathematical constant that takes on a value of about 2.7 and so $e^{-z}$ is that mathematical constant to the power of negative $Z$\n",
        "> NOTE: if $z$ were really big $e^{-z}$ is go to be a tiny tiny number\n",
        "\n",
        "> NOTE: if $z$ = 0 the $e^{-z}$ = $e^{0}$ which is equal to 1 so $g(z)$ = 0.5\n",
        "\n"
      ],
      "metadata": {
        "id": "Bs86LoRaxFqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's use this to build up to the logistic regression algorithm, we're going to do this in two steps:\n",
        "1. first you need to remember linear regression function :\\\n",
        "$f_{W,b}(X) = W.X + b$\\\n",
        "we can store this value in a variable which I'm going to call $z$ ans this will turn out to be tha same $z$ as the one you saw on on previous we taked.\n",
        "2. Next step is to take this value of $z$ and pass it to the sigmoid function (logistic function)\n",
        "\n",
        "Now if we put eveything next to other:\n",
        "> Logistic regression model:\\\n",
        "$f_{W,b}(X) = g(W.X + b) = \\frac{1}{1 + e^{-(W.X+b)}}$\\\n",
        "!! $(W.X+b) = z$\n",
        "\n",
        "What it does is it inputs a feature or set a feature x and it outputs a number between 0 and 1."
      ],
      "metadata": {
        "id": "GdaiCtXWmLCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take look at how to interpret the output of logistic regression:\n",
        "\n",
        "> MEMORY: $f_{W,b}(X) = \\frac{1}{1 + e^{-(W.X+b)}}$\n",
        "\n",
        "> NOTE: the way you to think of logistice regressions output is thick of it as outputting the probability that the cost or label y will be equal to 1 given a certain input X\n",
        "\n",
        "Let's continue with last example about tumor size:\\\n",
        "Example:\\\n",
        "$x$ is \"tumor size\"\\\n",
        "$y$ is 0 (not malignant) or 1 (malignant)\\\n",
        "\n",
        "if a patient come in and she\\he has a tumor of a certain size x:\\\n",
        "$f_{W,b}(X) = 0.7$\\\n",
        "that means is that the model is preficting or the model thinks there is a 70% chance that the true lable y will be equal to 1 for this patient\n",
        "\n",
        "so if y has a 70% chance of being one what is the chance that it is 0?\\\n",
        "> EXPLAIN: $P(y = 0) + P(y=1) = 1$\n",
        "\n",
        "so for our example the chance of it being zero has got to be 0.3 or 30% chance\n",
        "> $f_{W,b}(X) = P(y = 1|X;W,b)$\\\n",
        "; in here means W and b are parameters that affect this computation of what is the probability of Y being equal to 1 given the input feature X"
      ],
      "metadata": {
        "id": "8Ny_Q-KMrVuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Boundary"
      ],
      "metadata": {
        "id": "3uJNR6Bt6y80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Linear decision boundaries"
      ],
      "metadata": {
        "id": "p1XDzSNLSA0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> EXPLAIN: how logistic regression is computing is predictions\n",
        "\n",
        "for recap:\\\n",
        "$f_{W,b}(X)$\\\n",
        "$z = W . X + b$\\ -> Z -> $g(z) = \\frac{1}{1 + e^{-z}}$\\\n",
        "Another way to write this is:\\\n",
        "$f_{X,b}(X) = g(W.X + b)$\n",
        "\n",
        "> Formula:\\\n",
        "$f_{W,b}(X) = g(W.X + b) = \\frac{1}{1 + e^{-(W.X+b)}} = P(y = 1|x;W,b)$\\\n",
        "!! $(W.X+b) = z$\n",
        "\n",
        "What if you want the learning algorithm to predict is the value of y going to be 0 or 1?\\\n",
        "One way we can us threshold:\\\n",
        "Is $f_{W,b}(X) >= 0.5$ Yes:y-hat = 1/No:y-hat = 0\n",
        "\n",
        "When is $f_{W,b}(X) >= 0.5?$ it's happend when $g(z) >= 0.5$\\\n",
        "and it's happend when $z >= 0$ (when $z$ is on the right half of this axis)\\\n",
        "$z$ is greater than equal to zero whenever $(W.X + b) >= 0$\\\n",
        "what we told is the model predicts one\n",
        "\n",
        "If $W.X + b < 0$ the y-hat = 0\n"
      ],
      "metadata": {
        "id": "E11Kw23T66LZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's work with a example:\\\n",
        "Think we have a classification problem where you have two features X1 and X2\\\n",
        "We have a chart and both way until 3, we have training set with the little red crosses denote the positive examples and little blue circle denote negative examples\\\n",
        "so the red crosses corresponds to y = 1 and blue circles correspond to y = 0\\\n",
        "so the logistic regression model will make predictions using this function:\\\n",
        "$f_{W,b}(X) = g(z)$\\\n",
        "and now because of z:\\\n",
        "$f_{W,b}(X) = g(w_{1}x_{1} + w_{2}x_{2} + b)$\\\n",
        "Let's just say for this example that the value of the parameters are $w_{1}$ = 1, $w_{2} = 1$ and $b$ = -3\\\n",
        "Let's figure out when $(W.X + b)$ is greater than or equal to 0 and when $(W.X + b)$ is less than zero\n",
        "\n",
        "> NOTE: when $W.X + b$ is exactly equal to 0 it turns out that this line is also called the decision boundary because that's the line where you're just almost neutral about whether y is 0 or y is 1\n",
        "\n",
        "for the values of the parameters $w_{1}$ and $w_{2}$ and $b$ that we had written down above this decision boundary is just:\\\n",
        "$z = x_{1} + x_{2} - 3 = 0$\\\n",
        "that will correspond to the line:\\\n",
        "$x_{1} + x_{2} = 3$\\\n",
        "and that show the spefic line\n"
      ],
      "metadata": {
        "id": "0mzGqc4U-xPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Non-linear decision boundaries"
      ],
      "metadata": {
        "id": "0ZANmMV9SL9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at another example for Non-linear\\\n",
        "as befor crosses denote the class y = 1 and the little circles denote the clause y = 0\\\n",
        "we set z like this:\\\n",
        "$z = w_{1}x_{1}^{2} + w_{2}x_{2}^{2} + b$\n",
        "with this choice of features polynomial features into a logistic regression so:\\\n",
        "$f_{W,b}(X) = g(z) = g(w_{1}x_{1}^{2} + w_{2}x_{2}^{2} + b)$\\\n",
        "and let's say that we end up choosing $w_{1}$ and $w_{2}$ to be 1 and $b$ to be -1 so:\\\n",
        "$z = x_{1}^{2} + x_{2}^{2} - 1$\\\n",
        "and decision boundary as befor will correspond to when $z$ is equal to 0 and so this expression will be equal to 0 when $x_{1}^{2} + x_{2}^{2} = 1$\\\n",
        "When $x_{1}^{2} + x_{2}^{2} >= 1$ y-hat = 1\\\n",
        "When $x_{1}^{2} + x_{2}^{2} < 1$ y-hat = 0\n",
        "\n",
        "Also we can have more complex examples like:\\\n",
        "$f_{W,b}(X) = g(z) = g(w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{1}^{2} + w_{4}x_{1}x_{2} + w_{5}x_{2}^{2} + w_{6}x_{1}^{3} + ... + b)$\\\n",
        "when we create shape with that:\\\n",
        "y=hat = 1: means inside shape\\\n",
        "y-hat = 0: means outside shape\n"
      ],
      "metadata": {
        "id": "cpO7pyB6SQeM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gBSnlXuYS04O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cost Function"
      ],
      "metadata": {
        "id": "Oh96rpEzmcIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cost function for logistic regression"
      ],
      "metadata": {
        "id": "7bG3DSt4mfYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">REMEMBER: cost function gives you a way to measure how well a speific set of parameters fits the training data and it thereby gives you a way to try to choose better parameters\n",
        "\n",
        "! squared error cost function is not an Id cost function for religious regression\n",
        "\n",
        "Training set like that:\\\n",
        "Input's = tumor size, ..., patient's age\\\n",
        "Output's = malignant? (1:Yes/2:No)\n",
        "> NOTE:\\\n",
        "m = training examples (i = 1, ...,)\\\n",
        "n = features (j = 1, ...,)\\\n",
        "y = target, is 0 or 1\n",
        ">> Logistic regression model:\\\n",
        "$f_{w^{→},b}(x^{→}) = \\frac{1}{1 + e^{-(w^{→}.x^{→} + b)}}$\n",
        "\n",
        "How to choose $w^{→} = [w_{1}, w_{2}, w_{3}, ..., w_{n}]$ ?\n",
        "\n",
        "> MEMORY:\\\n",
        "Squared error cost:\\\n",
        "$J(w^{→},b) = \\frac{1}{m}Σ_{i = 1}^{m}\\frac{1}{2}(f_{w^{→},b}(x^{→(i)})-y^{i})^{2}$\\\n",
        "and we have linear regression:\\\n",
        "$f_{w^{→},b}(x^{→}) = (w^{→}.x^{→} + b)$\\\n",
        "and we take a one step one step and so on to converge at the global minimum\n",
        "\n",
        "but if you try **Squared error cost** you can see it dosen't works for **Logistic regression** and you recive to a non-convex cost function it's not context and it's means is that if you were to try to use gradient descent there are lot's of local minimum that you can get stuck in.\\\n",
        "We need to use different cost function that can make cost function convex again so the gradient descent can be guaranteed to converge to the global minimum.\\\n",
        "What we do:\\\n",
        "We're going to change a bit the definition of the cost function J of w and b\\\n",
        "We take inside of it and call it **loss**:\n",
        "loss[$L(\\frac{1}{2}(f_{w^{→},b}(x^{→(i)})-y^{(i)}$]\\\n",
        "The loss on a single trainig example and I'm going to denote the loss via this L and is a function of the prediction of the learning algorithm f of x as well as of the true label y.\n",
        "> NOTE: Chose different form for this loss function we'll be able to keep the overall cost function which is 1 over m times the sum of these loss functions to be a convex function\n"
      ],
      "metadata": {
        "id": "MvqO9zljmqK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of the loss function we'll use for logistic regression:\n",
        "> Formula:\\\n",
        "$L(f_{w^{→},b}(x^{→(i)}), y^{(i)} = $\\\n",
        "1: $-log(f_{w^{→},b}(x^{→(i)})$  if $y^{(i)} = 1$\\\n",
        "2: $-log(1 - f_{w^{→},b}(x^{→(i)})$  if $y^{(i)} = 0$\n",
        "\n",
        "Let's see what happend at case 1:\\\n",
        "$L(f_{w^{→},b}(x^{→(i)}), y^{(i)}$\\\n",
        "if $y^{i} = 1$\\\n",
        "As $f_{w^{→},b}(x^{→(i)})$ -> 1 then **loss** -> 0 🙂\\\n",
        "As $f_{w^{→},b}(x^{→(i)})$ -> 0 then **loss** -> ∞ 😞\n",
        "> NOTE: Loss is lowest when $f_{w^{→},b}(x^{→(i)})$ predict close to true label $y^{(i)}$\\\n",
        "Let's see what happend at case 2:\\\n",
        "$L(f_{w^{→},b}(x^{→(i)}), y^{(i)}$\\\n",
        "if $y^{i} = 0$\\\n",
        "As $f_{w^{→},b}(x^{→(i)})$ -> 0 then **loss** -> 0 🙂\\\n",
        "As $f_{w^{→},b}(x^{→(i)})$ -> 1 then **loss** -> ∞ 😞\\\n",
        "> NOTE: The further prediction $f_{w^{→},b}(x^{→(i)})$ is from target $y^{(i)}$, the higher the loss."
      ],
      "metadata": {
        "id": "pCdiDggzFqsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost:\\\n",
        "$J(w^{→},b) = \\frac{1}{m}Σ_{i = 1}^{m}L(f_{w^{→},b}(x^{→(i)}),y^{i})$\\\n",
        "**Loss** = $f_{w^{→},b}(x^{→(i)}),y^{i})$\\\n",
        "and it's equal with 2 cases:\\\n",
        "1: $-log(f_{w^{→},b}(x^{→(i)})$  if $y^{(i)} = 1$\\\n",
        "2: $-log(1 - f_{w^{→},b}(x^{→(i)})$  if $y^{(i)} = 0$\\\n",
        "and if you find the value of parameters w and b that minimize this then you'd have a pretty good set of values for the parameters w and b for logistic regression\n",
        "\n"
      ],
      "metadata": {
        "id": "2S3LYq8WJ_ZK"
      }
    }
  ]
}