{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNRqT7RamZG143SLZT3ffY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ErfanEbFiN79/Erfan-Python/blob/main/Python%20Erfan/Machine%20Learning/Start_01/Machine_Learning_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression Advance"
      ],
      "metadata": {
        "id": "bQLqPZsvvP-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear regression with multiple variables"
      ],
      "metadata": {
        "id": "_w9m6xqivV_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we just have a one input:\n",
        "> Formula: $f_{w,b}(x) = wx + b$\n",
        "\n",
        "x be a size of house and y is price"
      ],
      "metadata": {
        "id": "MC5PFE55yCqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "but what happed if we have more fetures like:\n",
        "* Number of bedrooms\n",
        "* Number of floors\n",
        "* Age of home in years"
      ],
      "metadata": {
        "id": "JQbV_ELzyZtk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Represent the list of features\n",
        "\n",
        "for represent we use:\\\n",
        "$x_{j} = j^{th}$ feature\\\n",
        "$n$ = number of features\\\n",
        "$x^{(i)}$ = feature of $j^{th}$ training example\\\n",
        "$x_{j}^{(i)}$ = value of feature $j$ in $i^{th}$ training example\n",
        "\n",
        "> NOTE: when we have multiple variables x means a vector of features that means: ($x_{1}, x_{2}, x_{3}, ..., x_{n}$)\n",
        "\n",
        "for example at house pricing if $n = 4$\n",
        "and we say $x^{(2)}$ that means $j = 2$ and mean in parentheses 2 will be a vector of the features for the second training example so:\\\n",
        "$x^{(2)} = [1416, 3, 2, 40]$\\\n",
        "And\n",
        "if we say $x_{3}^{(2)} = 2$ will be the value of the third figure, that is the number of flows in the secend training example and so that's going to be equal to 2."
      ],
      "metadata": {
        "id": "gLTa7npIy4Ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Previosly: $f_{w,b}(x) = wx + b$\n",
        "\n",
        "if we have 4 features:\n",
        "> $f_{w,b}(x) = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + w_{4}x_{4} + b$\n",
        "\n",
        "for example:\n",
        "> $f_{w,b}(x) = 0.1x_{1} + 4x_{2} + 10x_{3} + -2x_{4} + 80$\\\n",
        "$x_{1}$ is size\\\n",
        "$x_{2}$ is bedrooms\\\n",
        "$x_{3}$ is floors\\\n",
        "$x_{4}$ is years\\\n",
        "$b$ is base price\\\n",
        "and you can think 0.1 as saying that maybe for every additional square foot the price will increase by 0.1 thousand dollars or by a hundred dollars because we're saying that for each square foot the price inncreases by 0.1 you know times a thousand dollars which is a hundred dollars"
      ],
      "metadata": {
        "id": "NHkIswXs2b-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "1jGxkqEY8zdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in general if you have n features then the model will look like this:\n",
        "> Formula: $f_{w,b}(x) = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + ... + w_{n}x_{n} + b$\n",
        "\n",
        "Let's defin w as a list of numbers that list the parameters $[w_{1},w_{2},w_{3},...,w_{n}]$ in mathematics this is called a vector which just means a list of numbers.\\\n",
        "b is a single number.\\\n",
        "so the vector W together with this number B are the parameters of the model.\\\n",
        "! W is a row vectoe\\\n",
        "Also we can write X as a list or vector again in row vector that list all of the features $X = [X_{1}, X_{2}, X_{3}, ..., X_{n}]$\\\n",
        "! for show a vector you can add a arrow top of W and X"
      ],
      "metadata": {
        "id": "47GFkx664oke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on what we told at up now we have:\n",
        "> $f_{w,b}(X) = W.X + b = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + ... + w_{n}x_{n} + b$\n",
        "\n",
        "! dot(.) means dot product\n",
        "> Expline: dot product mean: well the dot products of two vectors of two list of numbers W and X is computed by talking the corresponding pairs of numbers."
      ],
      "metadata": {
        "id": "RGyx6j0c7QxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorization"
      ],
      "metadata": {
        "id": "lhLNCi02_4fb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "using vectorization will both make your code shorter and also make it run much more efficiently."
      ],
      "metadata": {
        "id": "kcBBf3bPADNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Parameters and features:\\\n",
        "$W = [w_{1}, w_{2}, w{3}]$\\\n",
        "b is a number\\\n",
        "$X = [x_{1}, x_{2}, x_{3}]$\\\n",
        "so here :\\\n",
        "$n = 3$\\\n",
        "! in linear algebra the index or the counting starts from one and so the first value is subscripted $w_{1}$ and $x_{1}$\n",
        "\n",
        "! **NumPy** is a numerical linear algebra and machine learning in python\\\n",
        "! in python we start from 0 so we can accsess to the first value with 0 and with 1 we access to secend element\n"
      ],
      "metadata": {
        "id": "dCJTDgc1AZYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "without vectorization:\n",
        "> $f_{w,b}(x) = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + b$\\\n",
        "At code:\\\n",
        "$f = w[0] * x[0] + w[1] * x[1] + w[2] + x[2] + b$\n",
        "\n",
        "! now n it's equal with 3, what happed if n = 100,000?\\\n",
        "we have a another way still without vectorization but by using a for loop\\\n",
        "in math:\\\n",
        "$f_{w,b}(X) = Σ_{j=1}^{n}w_{j}x_{j} + b$\\\n",
        "at code:\\\n",
        "f = 0\\\n",
        "for j in range(0,n):\\\n",
        "....f = f + w[j] * x [j]\\\n",
        "f = f + b"
      ],
      "metadata": {
        "id": "iGWjHtH8B4y3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "with Vectorization\n",
        "> $f_{w,b}(X) = W.X + b$\\\n",
        "code:\\\n",
        "f = np.dot(w,x) + b\n",
        "\n",
        "I want to emphasize that factorization actually has two distinct benefits:\n",
        "1. it makes the code shorter\n",
        "2. code running much faster"
      ],
      "metadata": {
        "id": "nQRvZxPNEEVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient descent for multiple regression"
      ],
      "metadata": {
        "id": "WtRawYV_e9jf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previous notation:\n",
        "> Parameters: $w_{1}, ..., w_{n}$ and $b$\\\n",
        "Model: $f_{w,b}(x) = w_{1}x_{1} + ... + w_{n}x_{n} + b$\\\n",
        "Cost function: $J(w_{1},...,w_{n},b)$\n",
        "Gradinet descent:\\\n",
        "repeat\\\n",
        "{\\\n",
        "$w_{j} = w_{j} - α\\frac{∂}{∂w_{j}}J(w_{1},...,w_{n},b)$\\\n",
        "$b = b - α\\frac{∂}{∂b}J(w_{1},...,w_{n},b)$\\\n",
        "}\n",
        "\n",
        "Vector notation:\n",
        "> Parameters: $W = [w_{1} ... w_{n}]$ and $b$\\\n",
        "Model: $f_{W,b}(X) = W.X + b$\\\n",
        "Cost function: $J(W,b)$\\\n",
        "Gradinet descent:\\\n",
        "repeat\\\n",
        "{\\\n",
        "$w_{j} = w_{j} - α\\frac{∂}{∂w_{j}}J(W,b)$\\\n",
        "$b = b - α\\frac{∂}{∂b}J(W,b)$\\\n",
        "}\n"
      ],
      "metadata": {
        "id": "qKgFl0S4fMOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's look at gradient descent"
      ],
      "metadata": {
        "id": "vo6srBGBnW0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> for one feature:\\\n",
        "repeat until convergence\\\n",
        "{\\\n",
        "$w = w - α\\frac{1}{m}Σ_{i=1}^{m}(f_{w,b}(x^{(i)}) - y^{i})x^{(i)}$\\\n",
        "$b = b - α\\frac{1}{m}Σ_{i=1}^{m}(f_{w,b}(x^{(i)}) - y^{i})$\\\n",
        "}\\\n",
        "simultaneously update $w,b$\\\n",
        "((!! $\\frac{1}{m}Σ_{i=1}^{m}(f_{w,b}(x^{(i)}) - y^{i})x^{(i)}$ = $\\frac{∂}{∂w}J(w,b)$))\n",
        "\n",
        "> $n$ features $n >= 2$:\\\n",
        "difference is that W and X are now vectors\n",
        "repeat until convergence\\\n",
        "{\\\n",
        "$w_{1} = w_{1} - α\\frac{1}{m}Σ_{i=1}^{m}(f_{W,b}(X^{(i)}) - y^{(i)})x_{1}^{(i)}$ => j = 1\\\n",
        "for multiple linear regression we have J ranging from 1 to n and so will update the parameters $w_{1}$ $w_{2}$ all the way up to $w_{n}$:\\\n",
        "$w_{n} = w_{n} - α\\frac{1}{m}Σ_{i=1}^{m}(f_{W,b}(X^{(i)}) - y^{(i)})x_{n}^{(i)}$ => j = n\\\n",
        "then as before we'll update b:\\\n",
        "$b = b - α\\frac{1}{m}Σ_{i=1}^{m}(f_{W,b}(X^{(i)}) - y^{(i)})$\\\n",
        "}\\\n",
        "simultaneously update: $w_{j}(for j = 1,..,n)$ and $b$"
      ],
      "metadata": {
        "id": "WjSyVVLWiHlz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### An alternative to gradient descent"
      ],
      "metadata": {
        "id": "8GMuR-GznZ4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "an alternative way for finding W and B for linear regression and this method is called the normal equation\\\n",
        "Benefits:\n",
        "* Only work for linear regression\n",
        "* Solve for w, b without iterations (not need an iterative gradient descent algorithm)\n",
        "\n",
        "Disadvantages:\n",
        "* Dosen't generalize to other learning algorithms.\n",
        "* Slow when number of features is large (> 10,000)\n",
        "\n",
        "What you need to know:\n",
        "* Normal equation method may be used in machine learning libraries that implement linear regression.\n",
        "* Gradient descent is the recommended method for finding parameters w,b"
      ],
      "metadata": {
        "id": "MlduMDttnlsx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partical Tips for Linear regression"
      ],
      "metadata": {
        "id": "HZXXAug-PPuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Scaling"
      ],
      "metadata": {
        "id": "CJil_Lc6PXLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's a techniques that make gradient descents work much better.\\\n",
        "It's will enable gradient descent to run mush faster\\\n",
        "Let's look at the relationship between the size of a feature that is how big are the numbers for that feature and the size of it's assicuated parameter as a concrete example look at\n",
        "> Example:\\\n",
        "price = $w_{1}x_{1} + w_{2}x_{2} + b$\\\n",
        "at here we have 2 features:\n",
        "> * $x_{1}$ size ($feet^{2}$) range: 300 - 2,000\n",
        "> * $x_{2}$ number of bedrooms range: 0 - 5\\\n",
        "\n",
        "Let's see a house:\\\n",
        "House: $x_{1}$ = 2000, $x_{2} = 5$, price = $500k\\\n",
        "\n",
        "Size of the parameters $w_{1},w_{2}$?\n",
        "1. $w_{1} = 50, w_{2} = 0.1, b = 50$\\\n",
        "price = 50 * 2000 + 0.1 * 5 + 50 => 100,050.0K\\\n",
        "so this is not a very good set of parameter choices for $w_{1}$ and $w_{2}$\n",
        "2. $w_{1} = 0.1, w_{2} = 50, b = 50$\\\n",
        "in here $w_{1}$ is relatively samll and $w_{2} is relatively large$\\\n",
        "price = 0.1 * 2000 + 50 * 5 + 50 => 100,050.0K\\\n",
        "in here price = 500k so it's more reasonable\n",
        "\n",
        "> EXPLAIN: when a possible range of values of a feature is large (like the size in square feet witch goes all the way up to 2000) that a good model will learn to choose a relatively small parameter value\\\n",
        "likewise when the possible values of a fearure are small then a resonable value for it's parameters will be relatively large\n",
        "\n",
        "rescaling are both now taking comparable ranges of values to each other and if you run gradient descent on a cost function"
      ],
      "metadata": {
        "id": "iWJ46sfFPf_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> EXPLAINE: when you have different features that take on very different ranges of values it can cause gradient descent to run slowly but rescaling the different features so they all take on comparable range of values can speed upgrade and descent significantly"
      ],
      "metadata": {
        "id": "Ff1ABxgOWtD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look how **Feature Scaling** take very different ranges of values and scale them to have comparable ranges\\\n",
        "How we get at feature scaling:\\\n",
        "if 300 <= $x_{1}$ <= 2000\\\n",
        "one way to get the scale version of $x_{1}$ is: $x_{1,scaled} = \\frac{x_{1}}{2000}$\\\n",
        "so the scale of $x_{1}$ will range :\\\n",
        "0.15 <= $w_{1,scaled}$ <= 1\n"
      ],
      "metadata": {
        "id": "nB3_P3sOtfHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean normalization"
      ],
      "metadata": {
        "id": "NTLGvTIbDypQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> EXPLAIN: You started the original feature and then you rescale them so that both of them are centerd around zero\n",
        "\n",
        "So whereas befor they only had values greater than zero now they have both negative and positive values but maybe usually between negative one and plus one\n",
        "\n",
        "How mean normalization work?\\\n",
        "1. Take avarage and show it with $μ$\n",
        "2. $x_{1} = \\frac{x_{1} - μ_{1}}{max - min}$\n",
        "3. $μ$ = 600 and max = 2000 and min 300\n",
        "4. with number 4 example -0.18 <= $x_{1}$ <= 0.82\n",
        "\n",
        "similarly to mean normalize $x_{2}$ you can calculate the avarage of feature 2 and for instance $μ_{2}$ may be 2.3 => 0 and 5\\\n",
        "then you can take each $x_{2}$:\\\n",
        "0 <= $x_{2}$ <= 5\\\n",
        "$x_{2} = \\frac{x_{2} - μ_{2}}{5-0}$\\\n",
        "so -0.46 <= $x_{2}$ <= 0.54"
      ],
      "metadata": {
        "id": "HgsfWJVQvYu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Z-score normalization"
      ],
      "metadata": {
        "id": "T7KctohuD6M_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to implement z-score normalization you need to calculate something called the standard deviation of each feature $σ$\\\n",
        "Way:\n",
        "> Calculate the mean $μ$ as well as the standard deviation which is often denoted by the $σ$ of each feature\\\n",
        "> EXAMPLE: if $σ$ = 450 and $m_{1}$ = 600\\\n",
        "then Z score normalize:\\\n",
        "$x_{1} = \\frac{x_{1} - μ_{1}}{σ_{1}}$\\\n",
        "and the result:\\\n",
        "-0.67 <= $x_{1}$ <= 3.1\n",
        "\n",
        "similary if you calculate the second feature's standard deviation to be 1.4 and mean to be 2.3\\\n",
        "then you can compute\\\n",
        "$x_{2} = \\frac{x_{2} - μ_{2}}{σ_{2}}$\\\n",
        "result:\\\n",
        "-1.6 <= $x_{2}$ <= 1.9\n",
        "\n",
        "Feature scaling\\\n",
        "aim for about -1 <= $x_{j}$ <= 1 for each feature $x_{j}$\\\n",
        "Acceptable range:\n",
        "* -3 <= $x_{j}$ <= 3\n",
        "* -0.3 <= $x_{j}$ <= 0.3\n",
        "\n",
        "Okay, no rescaling:\n",
        "* 0 <= $x_{1}$ <= 3\n",
        "* -2 <= $x_{2}$ <= 0.5\n",
        "\n",
        "Too large -> rescale:\n",
        "* -100 <= $x_{3}$ <= 100\n",
        "* 98.6 <= $x_{4}$ <= 105\n",
        "\n",
        "To Small -> rescale:\n",
        "* -0.0001 <= $x_{6}$ <= 0.001\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iwOHO3DtEBzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking Gradient descent for convergence"
      ],
      "metadata": {
        "id": "SAGQazvDd23T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How make sure gradient descent is working correctly?\\\n",
        "Let's again see the gradient descent:\\\n",
        "$w_{j} = w_{j} - α\\frac{∂}{∂w_{j}}J(W,b)$\\\n",
        "$b = b - α\\frac{∂}{∂b}J(W,b)$\\\n",
        "Something that I often do to make sure that gradient is working well recall the job of creating descent is to find parameters w and b that hopefully minimize the cost function J\\\n",
        "objective: $min_{W,b}J(W,b)$"
      ],
      "metadata": {
        "id": "_tzyieXOeFl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have a curve is also called a learning curve it's show number of iterations, note that there are a few different types of learning used in machine learning and you see some of the types later\n",
        "\n",
        "> NOTE: if gradient descent is working properly then the cost J should decrease after every single iteration if J ever invreases after one iteration that means either alpha ($α$) is chosen poorly and it usually means Alpha is too large or there could be a bug in the code\n",
        "\n",
        "after a time the cost J is leveling off and is no longer decreasing mush and by 400 iterations it looks like the curve has flattened out so this means that great and descent has more or less converged\n"
      ],
      "metadata": {
        "id": "RtgdUaR4kped"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to decide when your model is done training is with an automatic convergence test:\n",
        "> Automatic convergence test:\n",
        "Let 𝞮 \"epsilon\" be $10^{-3}$ be a variable representing a small number such as 0.0001 or 10 to the power of negative three\\\n",
        "if $J(W,b)$ decreases by <= $𝞮$ in one iteration, declare **convergence**\\\n",
        "found parameters $W,b$ to get close to global minimum\n"
      ],
      "metadata": {
        "id": "FPnBPxR1FvVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing The learning rate\n",
        "\n"
      ],
      "metadata": {
        "id": "BpqQUsc8ZqXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> NOTICE: learning rate if it's too samll would run very slowly and if it's too large it may not even coverge\n",
        "\n",
        "how we can choose a good learning rate for your model>\\\n",
        "! with a samll enough learning rate the cost function should decrease on every single iteration\\\n",
        "! if with a small $α$ J dosen't decrease on every single itation instead sometimes increases the that usually means there's a bug somewhere\\\n",
        "! if $α$ is too small then grade in descent can take a lot of iterations to converge\n",
        "\n",
        "you can try different values until found a value that's too small and then also make sure found a value too large, and slowly try to pick the largest possible leaning rate or just something slightly smaller than the largest resonable value that found"
      ],
      "metadata": {
        "id": "q7bE092ycpYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###"
      ],
      "metadata": {
        "id": "786YkZqZZoVk"
      }
    }
  ]
}