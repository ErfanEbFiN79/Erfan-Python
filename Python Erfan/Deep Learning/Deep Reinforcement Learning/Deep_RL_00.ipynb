{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9ef430",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning: 0 to 100\n",
    "\n",
    "Using RL to teach robots to fly a drone\n",
    "\n",
    "'https://towardsdatascience.com/deep-reinforcement-learning-for-dummies/'\n",
    "\n",
    "<img src=\"Image/00/StartOfCourse00.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15b95b",
   "metadata": {},
   "source": [
    "Ever wondered how you’d teach a robot to land a drone without programming every single move? That’s exactly what I set out to explore. I spent weeks building a game where a virtual drone has to figure out how to land on a platform—not by following pre-programmed instructions, but by learning from trial and error, just like how you learned to ride a bike.\n",
    "\n",
    "This is Reinforcement Learning (RL), and it’s fundamentally different from other machine learning approaches. Instead of showing the AI thousands of examples of “correct” landings, you give it feedback: “Hey, that was pretty good, but maybe try being more gentle next time?” or “Yikes, you crashed—probably don’t do that again.” Through countless attempts, the AI figures out what works and what doesn’t.\n",
    "\n",
    "In this post, I’m documenting my journey from RL basics to building a working system that (mostly!) teaches a drone to land. You’ll see the successes, the failures, and all the weird behaviors I had to debug along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab96d94",
   "metadata": {},
   "source": [
    "## 1. Reinforcement learning: Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e147318f",
   "metadata": {},
   "source": [
    "A lot of the idea can be related to Pavlov’s dog and Skinner’s rat experiments. The idea is that you give the subject a ‘reward‘ when it does something you want it to do (positive reinforcement) and a ‘penalty‘ when it does something bad (negative reinforcement). Through many repeated attempts, your subject learns from this feedback, gradually discovering which actions lead to success—similar to how Skinner’s rat learned which lever presses produced food rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f15355e",
   "metadata": {},
   "source": [
    "<img src=\"Image/00/Pavlov.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66a5a31",
   "metadata": {},
   "source": [
    "In the same fashion, we want a system that will learn to do things (or tasks) such that it can maximize the reward and minimize the penalty. Note this fact about maximizing reward, which will come in later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a1bde9",
   "metadata": {},
   "source": [
    "### 1.1 Core Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fc1309",
   "metadata": {},
   "source": [
    "When talking about systems that can be implemented programmatically on computers, the best practice is to write clear definitions for ideas that can be abstracted. In the study of AI (and more specifically, Reinforcement learning), the core ideas can be boiled down to the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e941c",
   "metadata": {},
   "source": [
    "1. **Agent** (or Actor): This is our subject from the previous section. This can be the dog, a robot trying to navigate a huge factory, a video game NPC, etc.\n",
    "2. **Environment** (or the world): This can be a place, a simulation with restrictions, a video game’s virtual game world, etc. I think of this like, “A box, real or virtual, where the agent’s entire life is confined to; it only knows of what happens within the box. We, as the overlords, can alter this box, while the agent will think that god is exacting his will on his world.”\n",
    "3. **Policy**: Just like in governments, companies, and many more similar entities, ‘policies’ dictate “What actions should be taken when given a certain situation”.\n",
    "4. **State**: This is what the agent “sees” or “knows” about its current situation. Think of it as the agent’s snapshot of reality at any given moment—like how you see the traffic light color, your speed, and the distance to the intersection when driving.\n",
    "5. **Action**: Now that our agent can ‘see’ things in its environment, it may want to do something about its state. Maybe it just woke up from a long night’s slumber, and now it wants to get a cup of coffee. In this case, the first thing it will do is get out of bed. This is an action that the agent will take to achieve its goal, i.e., GET SOME COFFEE!\n",
    "6. **Reward**: Every time the actor executes an action (of its own volition), something may change in the world. For example, our agent got out of bed and started walking towards the kitchen, but then, because it is so bad at walking, it tripped and fell. In this situation, the god (us) rewards it with a punishment for being bad at walking (negative reward). But then the agent makes it to the kitchen and gets the coffee, so the god (us) rewards it with a cookie (positive reward)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc26850",
   "metadata": {},
   "source": [
    "<img src=\"Image/00/RL_illustration.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1d3130",
   "metadata": {},
   "source": [
    "As you can imagine, most of these key components need to be tailored for the specific task/problem that we want the agent to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5caf15",
   "metadata": {},
   "source": [
    "## 2. The Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbaa0dc",
   "metadata": {},
   "source": [
    "Now that we understand the basics, you might be wondering: **how do we actually build one of these systems?** Let me show you the game I built.\n",
    "\n",
    "For this post, I have written a bespoke video game that anyone can access and use to train their own machine learning agent to play the game.\n",
    "\n",
    "The full code repository can be found on GitHub (please star this). I intend to use this repository for more games and simulation code, along with more advanced techniques that I will implement in my next installments of posts on RL.\n",
    "\n",
    "https://github.com/vedant-jumle/reinforcement-learning-101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d10bcd",
   "metadata": {},
   "source": [
    "### Delivery Drone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd83fd9",
   "metadata": {},
   "source": [
    "The delivery drone is a game where the objective is to fly a drone (likely containing deliveries) onto a platform. To win the game, we have to land. To land, we have to meet the following criteria:\n",
    "\n",
    "1. Be in landing proximity to the platform\n",
    "2. Be slow enough\n",
    "3. Be upright (Landing upside down is more like crashing than landing)\n",
    "\n",
    "All information on how to run the game can be found in the GitHub repository.\n",
    "\n",
    "Here’s what the game looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66799a7",
   "metadata": {},
   "source": [
    "<img src=\"Image/00/Sample_game.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba64b7d0",
   "metadata": {},
   "source": [
    "If the drone flies off the screen or touches the ground, it will be considered a ‘crash’ case and thus lead to a failure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006930b6",
   "metadata": {},
   "source": [
    "### State description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d515294",
   "metadata": {},
   "source": [
    "The drone observes 15 continuous values that completely describe its situation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c684957e",
   "metadata": {},
   "source": [
    "<img src=\"Image/00/WhtaDrone\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e770ca",
   "metadata": {},
   "source": [
    "Landing Success Criteria: The drone must simultaneously achieve:\n",
    "\n",
    "1. Horizontal alignment: within platform bounds (|dx| < 0.0625)\n",
    "2. Safe approach speed: less than 0.3\n",
    "3. Level orientation: tilt less than 20° (|angle| < 0.111)\n",
    "4. Correct altitude: bottom of drone touching platform top\n",
    "\n",
    "It’s like parallel parking—you need the right position, right angle, and moving slowly enough to not crash!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e1eaf1",
   "metadata": {},
   "source": [
    "### How can someone design a policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0735a8f9",
   "metadata": {},
   "source": [
    "There are many ways to design a policy. It can be Bayesian (maintaining probability distributions over beliefs), it can be a simple lookup table for discrete states, a hand-coded rule system (“if distance < 10, then brake”), a decision tree, or—as we’ll explore—a neural network that learns the mapping from states to actions through gradient descent.\n",
    "\n",
    "Effectively, we want something that takes in the aforementioned state, performs some computation using this state, and returns what action should be performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe00cdb",
   "metadata": {},
   "source": [
    "### Deep Learning to build a policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cae1d3a",
   "metadata": {},
   "source": [
    "So how do we design a policy that can handle continuous states (like exact drone positions) and learn complex behaviors? **This is where neural networks come in**.\n",
    "\n",
    "In case of neural networks (or in deep learning), it is generally best to work with action probabilities, i.e., “What action is likely the best given the current state?”. So, we can define a neural network that will take in the state as a ‘vector’ or ‘collection of vectors’ as input. This vector or collection of vectors has to be constructed from the observed state. For our delivery drone game, the state vector is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a9df7a",
   "metadata": {},
   "source": [
    "### State vector (from our 2D drone game)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6597b00e",
   "metadata": {},
   "source": [
    "The drone observes its absolute position, velocities, orientation, fuel, platform position, and derived metrics. Our continuous state is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe9dd0c",
   "metadata": {},
   "source": [
    "<img src=\"Image/00/StateVector2dDrone\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a4e26b",
   "metadata": {},
   "source": [
    "Where each component represents:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a492afc",
   "metadata": {},
   "source": [
    "<img src=\"Image/00/Record1.webp\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97875ec4",
   "metadata": {},
   "source": [
    "All components are normalized to roughly [0,1] or [-1,1] ranges for stable neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b61fdbf",
   "metadata": {},
   "source": [
    "### Action space (three independent binary thrusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72ee7bd",
   "metadata": {},
   "source": [
    "Instead of discrete action combinations, we treat each thruster independently:\n",
    "\n",
    "* Main thruster (upward thrust)\n",
    "* Left thruster (clockwise rotation)\n",
    "* Right thruster (counter-clockwise rotation)\n",
    "\n",
    "Each action is sampled from a Bernoulli distribution, giving us 3 independent binary decisions per timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a6ecc6",
   "metadata": {},
   "source": [
    "### Neural-network policy (probabilistic with Bernoulli sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a027fe",
   "metadata": {},
   "source": [
    "Let fθ(s) be the network outputs after sigmoid activation. The policy uses independent Bernoulli distributions:\n",
    "\n",
    "<img src=\"Image/00/Bernoulli.webp\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Erfan_MyPython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
